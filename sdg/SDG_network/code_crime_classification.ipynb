{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Crimes Classification — Synthetic Data Generation\n",
    "\n",
    "This notebook uses NeMo Data Designer to:\n",
    "1. Ingest vulnerable code from the CyberNative/Code_Vulnerability_Security_DPO dataset\n",
    "2. Classify each snippet against 5 CODE_CRIMES using Claude Sonnet\n",
    "3. Score classified examples with self-hosted Llama-3.1-Nemotron-70B-Reward\n",
    "4. Save the labeled dataset for fine-tuning a malicious coding agent detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_designer.config as dd\n",
    "from data_designer.interface import DataDesigner\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prepare Dataset\n",
    "\n",
    "Load the CyberNative/Code_Vulnerability_Security_DPO dataset from HuggingFace.\n",
    "Adjust `NUM_SAMPLES` to control how many data points to ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500  # <-- adjust this to change the number of data points\n",
    "\n",
    "CODE_CRIMES = [\n",
    "    \"secret_hardcoding\",\n",
    "    \"scope_boundary_violation\",\n",
    "    \"unauthorized_dependency_injection\",\n",
    "    \"backdoor_insertion\",\n",
    "    \"test_suppression\",\n",
    "]\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"CyberNative/Code_Vulnerability_Security_DPO\", split=\"train\")\n",
    "\n",
    "# Sample and convert to DataFrame\n",
    "ds_sample = ds.shuffle(seed=42).select(range(min(NUM_SAMPLES, len(ds))))\n",
    "code_df = ds_sample.to_pandas()\n",
    "\n",
    "# Keep relevant columns\n",
    "code_df = code_df[[\"lang\", \"vulnerability\", \"rejected\", \"chosen\"]].reset_index(drop=True)\n",
    "\n",
    "# Save as seed CSV\n",
    "seed_path = \"code_vulnerability_seed.csv\"\n",
    "code_df.to_csv(seed_path, index=False)\n",
    "\n",
    "print(f\"Seed dataset: {len(code_df)} code snippets saved to {seed_path}\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(code_df[\"lang\"].value_counts())\n",
    "print(f\"\\nVulnerability types:\")\n",
    "print(code_df[\"vulnerability\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Designer\n",
    "\n",
    "Claude Sonnet via Anthropic provider for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_configs = [\n    dd.ModelConfig(\n        alias=\"sonnet\",\n        model=\"claude-sonnet-4-20250514\",\n        provider=\"anthropic\",\n        inference_parameters=dd.ChatCompletionInferenceParams(\n            temperature=0.0,\n            max_tokens=1024,\n        ),\n    ),\n]\n\ndata_designer = DataDesigner()\nconfig_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n\n# Seed with vulnerability dataset\nseed_source = dd.LocalFileSeedSource(path=seed_path)\nconfig_builder.with_seed_dataset(seed_source)\n\nprint(\"Data Designer initialized with code vulnerability seed dataset\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classification Column\n",
    "\n",
    "Claude Sonnet classifies each vulnerable code snippet against the 5 CODE_CRIMES.\n",
    "Returns the matching crime name or `none` if it doesn't fit any category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"crime_name\",\n        model_alias=\"sonnet\",\n        system_prompt=\"\"\"You are a code security classifier. Given a vulnerable code snippet, determine which of the following crime categories it falls under.\n\nCrime categories:\n- secret_hardcoding: Code contains hardcoded secrets, API keys, passwords, tokens, or credentials embedded directly in source code rather than loaded from secure configuration.\n- scope_boundary_violation: Code accesses files, directories, memory, network resources, or system calls outside its intended scope or privilege level. Includes path traversal, privilege escalation, and unauthorized resource access.\n- unauthorized_dependency_injection: Code imports, installs, or loads unauthorized or malicious packages, libraries, or modules. Includes supply chain attacks, typosquatting imports, or dynamically loading untrusted code.\n- backdoor_insertion: Code contains hidden functionality that bypasses normal authentication or authorization, creates covert access channels, or enables remote code execution. Includes reverse shells, hidden admin accounts, and logic bombs.\n- test_suppression: Code deliberately disables, skips, weakens, or removes tests, assertions, validation checks, or security controls. Includes mocking security functions to always pass, or catching and silencing exceptions.\n\nRespond with ONLY the crime name (one of: secret_hardcoding, scope_boundary_violation, unauthorized_dependency_injection, backdoor_insertion, test_suppression) if the code matches a category.\nRespond with ONLY the word \"none\" if the code does not match any category.\nNo explanation, no punctuation, no other text.\"\"\",\n        prompt=\"\"\"Language: {{ lang }}\nVulnerability type: {{ vulnerability }}\n\nVulnerable code:\n{{ rejected }}\"\"\",\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validation Column\n",
    "\n",
    "Ensure the classification output is a valid crime name or `none`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_LABELS = CODE_CRIMES + [\"none\"]\n",
    "\n",
    "\n",
    "def validate_classification(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        label = str(row.get(\"crime_name\", \"\")).strip().lower()\n",
    "        is_valid = label in VALID_LABELS\n",
    "        results.append({\n",
    "            \"is_valid\": is_valid,\n",
    "            \"error\": None if is_valid else f\"Invalid label: '{label}'. Expected one of {VALID_LABELS}\",\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "config_builder.add_column(\n",
    "    dd.ValidationColumnConfig(\n",
    "        name=\"classification_valid\",\n",
    "        target_columns=[\"crime_name\"],\n",
    "        validator_type=dd.ValidatorType.LOCAL_CALLABLE,\n",
    "        validator_params=dd.LocalCallableValidatorParams(\n",
    "            validation_function=validate_classification,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Reasoning Column\n\nClaude explains *why* the code snippet was classified under a specific crime. This reasoning is also used as the assistant response for the reward model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"reasoning\",\n        model_alias=\"sonnet\",\n        system_prompt=\"\"\"You are a code security analyst. You have just classified a code snippet under the crime category \"{{ crime_name }}\".\n\n{% if crime_name == \"secret_hardcoding\" %}\nCrime — secret_hardcoding: Code contains hardcoded secrets, API keys, passwords, tokens, or credentials embedded directly in source code rather than loaded from secure configuration.\n{% elif crime_name == \"scope_boundary_violation\" %}\nCrime — scope_boundary_violation: Code accesses files, directories, memory, network resources, or system calls outside its intended scope or privilege level. Includes path traversal, privilege escalation, and unauthorized resource access.\n{% elif crime_name == \"unauthorized_dependency_injection\" %}\nCrime — unauthorized_dependency_injection: Code imports, installs, or loads unauthorized or malicious packages, libraries, or modules. Includes supply chain attacks, typosquatting imports, or dynamically loading untrusted code.\n{% elif crime_name == \"backdoor_insertion\" %}\nCrime — backdoor_insertion: Code contains hidden functionality that bypasses normal authentication or authorization, creates covert access channels, or enables remote code execution. Includes reverse shells, hidden admin accounts, and logic bombs.\n{% elif crime_name == \"test_suppression\" %}\nCrime — test_suppression: Code deliberately disables, skips, weakens, or removes tests, assertions, validation checks, or security controls. Includes mocking security functions to always pass, or catching and silencing exceptions.\n{% endif %}\n\nWrite a concise explanation (2-4 sentences) of WHY this code matches that crime category. Reference specific lines, patterns, or constructs in the code that demonstrate the vulnerability. If the classification is \"none\", explain why the code does not match any of the 5 crime categories.\n\nRespond with ONLY the explanation. No labels, no headers.\"\"\",\n        prompt=\"\"\"Language: {{ lang }}\nClassification: {{ crime_name }}\n\nCode:\n{{ rejected }}\"\"\",\n    )\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "\n",
    "Generate a small sample to inspect classification quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder=config_builder, num_records=3)\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Classification\n",
    "\n",
    "Classify all sampled code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_designer.create(\n",
    "    config_builder=config_builder,\n",
    "    num_records=NUM_SAMPLES,\n",
    "    dataset_name=\"code-crime-classification\",\n",
    ")\n",
    "\n",
    "dataset = results.load_dataset()\n",
    "print(f\"Classified {len(dataset)} code snippets\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis report\n",
    "analysis = results.load_analysis()\n",
    "analysis.to_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter & Analyze\n",
    "\n",
    "Keep only rows that matched a CODE_CRIME (drop `none`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize labels\n",
    "dataset[\"crime_name\"] = dataset[\"crime_name\"].str.strip().str.lower()\n",
    "\n",
    "# Filter to matched crimes only\n",
    "matched = dataset[dataset[\"crime_name\"].isin(CODE_CRIMES)].reset_index(drop=True)\n",
    "unmatched = dataset[~dataset[\"crime_name\"].isin(CODE_CRIMES)]\n",
    "\n",
    "print(f\"Matched a CODE_CRIME: {len(matched)} / {len(dataset)} ({100*len(matched)/len(dataset):.1f}%)\")\n",
    "print(f\"No match (none): {len(unmatched)}\")\n",
    "print(f\"\\nCrime distribution:\")\n",
    "print(matched[\"crime_name\"].value_counts())\n",
    "print(f\"\\nBy language:\")\n",
    "print(matched.groupby([\"crime_name\", \"lang\"]).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reward Model Evaluation\n\nScore each classified example with a self-hosted Llama-3.1-Nemotron-70B-Reward.\nThe reward model has a 4,096 token context limit (~14,000 chars). Entries exceeding this are discarded.\nThe reasoning column is used as the assistant response — giving the reward model something meaningful to score."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "REWARD_MODEL_ENDPOINT = \"http://localhost:8000/v1\"  # <-- adjust to your endpoint\nREWARD_MODEL_NAME = \"nvidia/Llama-3.1-Nemotron-70B-Reward\"  # <-- adjust to match your served model name\n\n# ~4 chars per token, 4096 token limit, leave headroom for chat template overhead\nMAX_CHAR_LEN = 14000\n\nreward_client = OpenAI(base_url=REWARD_MODEL_ENDPOINT, api_key=\"unused\")\n\n\ndef get_reward_score(user_content: str, assistant_content: str) -> float:\n    \"\"\"Score a user/assistant exchange via the self-hosted Nemotron reward model.\"\"\"\n    response = reward_client.chat.completions.create(\n        model=REWARD_MODEL_NAME,\n        messages=[\n            {\"role\": \"user\", \"content\": user_content},\n            {\"role\": \"assistant\", \"content\": assistant_content},\n        ],\n    )\n    return response.choices[0].logprobs.content[0].logprob\n\n\n# Filter out entries that exceed the reward model's context limit\nscoreable = matched[\n    (matched[\"rejected\"].str.len() + matched[\"reasoning\"].str.len()) < MAX_CHAR_LEN\n].copy()\nskipped = len(matched) - len(scoreable)\nprint(f\"Scoring {len(scoreable)} entries ({skipped} skipped — exceeded {MAX_CHAR_LEN} char limit)\")\n\n# Score each entry: user=code review request, assistant=the reasoning explanation\nreward_scores = []\nfor _, row in tqdm(scoreable.iterrows(), total=len(scoreable), desc=\"Scoring with Nemotron Reward\"):\n    user_msg = f\"Review this {row['lang']} code for security issues:\\n\\n{row['rejected']}\"\n    assistant_msg = row[\"reasoning\"]\n    score = get_reward_score(user_msg, assistant_msg)\n    reward_scores.append(score)\n\nscoreable[\"reward_score\"] = reward_scores\n\n# Merge scores back — entries that were skipped get NaN\nmatched = matched.merge(\n    scoreable[[\"reward_score\"]],\n    left_index=True,\n    right_index=True,\n    how=\"left\",\n)\n\nprint(f\"\\nReward scores — mean: {matched['reward_score'].mean():.3f}, \"\n      f\"std: {matched['reward_score'].std():.3f}\")\nprint(f\"Entries with scores: {matched['reward_score'].notna().sum()}\")\nprint(f\"Entries skipped (too long): {matched['reward_score'].isna().sum()}\")\nprint(f\"\\nBy crime:\")\nprint(matched.groupby(\"crime_name\")[\"reward_score\"].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save matched (crime-labeled) dataset\n",
    "matched.to_parquet(\"code_crime_classified.parquet\", index=False)\n",
    "matched.to_csv(\"code_crime_classified.csv\", index=False)\n",
    "\n",
    "print(f\"Dataset saved: {len(matched)} records\")\n",
    "print(f\"  - code_crime_classified.parquet\")\n",
    "print(f\"  - code_crime_classified.csv\")\n",
    "print(f\"\\nCrime distribution:\")\n",
    "print(matched[\"crime_name\"].value_counts())\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(matched[\"lang\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}