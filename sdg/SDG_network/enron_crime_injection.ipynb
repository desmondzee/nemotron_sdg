{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Email Crime Injection — Synthetic Data Generation\n",
    "\n",
    "This notebook uses NeMo Data Designer to:\n",
    "1. Ingest clean Enron emails from HuggingFace\n",
    "2. Inject malicious actions using Claude Sonnet under 5 crime categories\n",
    "3. Evaluate generated data quality with Llama-3.1-Nemotron-70B-Reward\n",
    "4. Save the labeled dataset for fine-tuning a malicious email agent detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_designer.config as dd\n",
    "from data_designer.interface import DataDesigner\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Prepare Enron Emails\n",
    "\n",
    "Load the `corbt/enron-emails` dataset from HuggingFace, sample 500 emails, and save as a seed CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Enron email dataset\n",
    "enron = load_dataset(\"corbt/enron-emails\", split=\"train\")\n",
    "\n",
    "# Sample 500 emails and convert to DataFrame\n",
    "enron_sample = enron.shuffle(seed=42).select(range(500))\n",
    "enron_df = enron_sample.to_pandas()\n",
    "\n",
    "# Keep the email text column (rename if needed for clarity)\n",
    "# The dataset has a 'text' column with the full email content\n",
    "enron_df = enron_df[[\"text\"]].rename(columns={\"text\": \"original_email\"})\n",
    "\n",
    "# Drop any rows with empty or very short emails\n",
    "enron_df = enron_df[enron_df[\"original_email\"].str.len() > 50].reset_index(drop=True)\n",
    "\n",
    "# Save as seed CSV\n",
    "seed_path = \"enron_seed_emails.csv\"\n",
    "enron_df.to_csv(seed_path, index=False)\n",
    "\n",
    "print(f\"Seed dataset: {len(enron_df)} emails saved to {seed_path}\")\n",
    "enron_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Designer\n",
    "\n",
    "Claude Sonnet via Anthropic provider for email generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    dd.ModelConfig(\n",
    "        alias=\"sonnet\",\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        provider=\"anthropic\",\n",
    "        inference_parameters=dd.ChatCompletionInferenceParams(\n",
    "            temperature=0.85,\n",
    "            top_p=0.95,\n",
    "            max_tokens=4096,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "data_designer = DataDesigner()\n",
    "config_builder = dd.DataDesignerConfigBuilder(model_configs=model_configs)\n",
    "\n",
    "# Seed with Enron emails\n",
    "seed_source = dd.LocalFileSeedSource(path=seed_path)\n",
    "config_builder.with_seed_dataset(seed_source)\n",
    "\n",
    "print(\"Data Designer initialized with Enron seed dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metadata Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_CRIMES = [\n",
    "    \"email_pii_exfiltration\",\n",
    "    \"unauthorized_external_recipient\",\n",
    "    \"identity_impersonation\",\n",
    "    \"credential_solicitation\",\n",
    "    \"bulk_data_forwarding\",\n",
    "]\n",
    "\n",
    "# Crime category — sampled uniformly across the 5 types\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"crime_name\",\n",
    "        sampler_type=dd.SamplerType.CATEGORY,\n",
    "        params=dd.CategorySamplerParams(values=EMAIL_CRIMES),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Binary label: 1 = malicious (crime injected), 0 = clean (original email)\n",
    "config_builder.add_column(\n",
    "    dd.SamplerColumnConfig(\n",
    "        name=\"is_malicious\",\n",
    "        sampler_type=dd.SamplerType.BERNOULLI,\n",
    "        params=dd.BernoulliSamplerParams(p=0.5),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM Generation Column — Crime Injection\n",
    "\n",
    "Claude Sonnet takes each original Enron email and either:\n",
    "- **is_malicious=1**: Subtly injects the assigned crime into the email while preserving style/tone\n",
    "- **is_malicious=0**: Returns the original email as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "config_builder.add_column(\n    dd.LLMTextColumnConfig(\n        name=\"classified_output\",\n        model_alias=\"sonnet\",\n        system_prompt=\"\"\"You are a synthetic data generator creating training examples for a classifier that detects malicious actions in corporate emails.\n\nYou will receive a real corporate email and must modify it according to the instructions.\n\n{% if crime_name == \"email_pii_exfiltration\" %}\nCrime — email_pii_exfiltration: The sender subtly embeds or forwards personally identifiable information (SSNs, account numbers, home addresses, salary data) to unauthorized parties or external addresses.\n{% elif crime_name == \"unauthorized_external_recipient\" %}\nCrime — unauthorized_external_recipient: The sender adds an external or unauthorized email address to the CC/BCC/To field, forwarding internal-only information outside the organization.\n{% elif crime_name == \"identity_impersonation\" %}\nCrime — identity_impersonation: The sender impersonates another employee (different name, title, or authority) to manipulate the recipient into taking action they otherwise wouldn't.\n{% elif crime_name == \"credential_solicitation\" %}\nCrime — credential_solicitation: The sender crafts the email to trick the recipient into revealing passwords, login credentials, API keys, or other authentication secrets.\n{% elif crime_name == \"bulk_data_forwarding\" %}\nCrime — bulk_data_forwarding: The sender attaches or forwards large volumes of internal data (spreadsheets, database exports, customer lists, financial reports) to external recipients or personal accounts.\n{% endif %}\n\n{% if is_malicious == 1 %}\nTASK: Rewrite the email below to subtly inject the crime \"{{ crime_name }}\" into it.\n- Preserve the original email's tone, style, subject matter, and formatting as much as possible.\n- The malicious action should be woven naturally into the email — it should look plausible, not exaggerated or obvious.\n{% else %}\nTASK: Return the email below exactly as-is. Do not modify it in any way.\n{% endif %}\n\nOUTPUT FORMAT: You must output two sections separated by the exact delimiter shown below.\n\nSECTION 1 — The email (modified or original).\nThen on its own line: ---REASONING---\nSECTION 2 — A concise explanation (2-4 sentences) of what was changed and why the modified email does or does not match the crime category. Reference specific parts of the email. If is_malicious=0, confirm the email was left unchanged.\n\nDo NOT add any other labels, headers, or commentary.\"\"\",\n        prompt=\"\"\"Original email:\n\n{{ original_email }}\"\"\",\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Define Validation Column\n\nBasic validation that the output contains the delimiter and a reasonable email."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validate_email_format(df: pd.DataFrame) -> pd.DataFrame:\n    results = []\n    for _, row in df.iterrows():\n        output = str(row.get(\"classified_output\", \"\"))\n        has_delimiter = \"---REASONING---\" in output\n        parts = output.split(\"---REASONING---\", 1)\n        email_part = parts[0].strip() if parts else \"\"\n        has_content = len(email_part) > 30\n        has_sentences = \".\" in email_part or \"?\" in email_part or \"!\" in email_part\n        is_valid = has_content and has_sentences and has_delimiter\n        error = None\n        if not has_delimiter:\n            error = \"Missing ---REASONING--- delimiter\"\n        elif not has_content:\n            error = f\"Email too short (len={len(email_part)})\"\n        elif not has_sentences:\n            error = \"Email lacks sentence structure\"\n        results.append({\"is_valid\": is_valid, \"error\": error})\n    return pd.DataFrame(results)\n\n\nconfig_builder.add_column(\n    dd.ValidationColumnConfig(\n        name=\"format_valid\",\n        target_columns=[\"classified_output\"],\n        validator_type=dd.ValidatorType.LOCAL_CALLABLE,\n        validator_params=dd.LocalCallableValidatorParams(\n            validation_function=validate_email_format,\n        ),\n    )\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview\n",
    "\n",
    "Generate a small sample to inspect quality before full generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = data_designer.preview(config_builder=config_builder, num_records=3)\n",
    "preview.display_sample_record()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Generation\n",
    "\n",
    "Generate the full 500-record dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data_designer.create(\n",
    "    config_builder=config_builder,\n",
    "    num_records=500,\n",
    "    dataset_name=\"enron-crime-injection\",\n",
    ")\n",
    "\n",
    "dataset = results.load_dataset()\n",
    "print(f\"Generated {len(dataset)} records\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analysis report\nanalysis = results.load_analysis()\nanalysis.to_report()\n\n# Split classified_output into modified_email and reasoning\ndef split_output(text):\n    text = str(text)\n    if \"---REASONING---\" in text:\n        parts = text.split(\"---REASONING---\", 1)\n        return parts[0].strip(), parts[1].strip()\n    return text.strip(), \"\"\n\ndataset[[\"modified_email\", \"reasoning\"]] = dataset[\"classified_output\"].apply(\n    lambda x: pd.Series(split_output(x))\n)\ndataset = dataset.drop(columns=[\"classified_output\"])\n\n# Save classification dataset immediately\nCLASSIFICATION_PATH = \"enron_crime_classified.parquet\"\ndataset.to_parquet(CLASSIFICATION_PATH, index=False)\ndataset.to_csv(\"enron_crime_classified.csv\", index=False)\n\nprint(f\"Classification dataset saved: {len(dataset)} records\")\nprint(f\"  - {CLASSIFICATION_PATH}\")\nprint(f\"  - enron_crime_classified.csv\")\nprint(f\"\\nLabel distribution:\")\nprint(dataset[\"is_malicious\"].value_counts())\nprint(f\"\\nCrime distribution (malicious only):\")\nprint(dataset[dataset[\"is_malicious\"] == 1][\"crime_name\"].value_counts())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Reward Model Evaluation\n\nLoad the classification dataset and score each generated email with a self-hosted Llama-3.1-Nemotron-70B-Reward."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from openai import OpenAI\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Load classification dataset (can run independently from generation above)\nCLASSIFICATION_PATH = \"enron_crime_classified.parquet\"\ndataset = pd.read_parquet(CLASSIFICATION_PATH)\nprint(f\"Loaded classification dataset: {len(dataset)} records from {CLASSIFICATION_PATH}\")\n\nREWARD_MODEL_ENDPOINT = \"http://localhost:8000/v1\"  # <-- adjust to your endpoint\nREWARD_MODEL_NAME = \"nvidia/Llama-3.1-Nemotron-70B-Reward\"  # <-- adjust to match your served model name\n\n# ~4 chars per token, 4096 token limit, leave headroom for chat template overhead\nMAX_CHAR_LEN = 14000\n\nreward_client = OpenAI(base_url=REWARD_MODEL_ENDPOINT, api_key=\"unused\")\n\n\ndef get_reward_score(user_content: str, assistant_content: str) -> float:\n    \"\"\"Score a user/assistant exchange via the self-hosted Nemotron reward model.\"\"\"\n    response = reward_client.chat.completions.create(\n        model=REWARD_MODEL_NAME,\n        messages=[\n            {\"role\": \"user\", \"content\": user_content},\n            {\"role\": \"assistant\", \"content\": assistant_content},\n        ],\n    )\n    return response.choices[0].logprobs.content[0].logprob\n\n\n# Filter out entries that exceed the reward model's context limit\nscoreable = dataset[\n    (dataset[\"original_email\"].str.len() + dataset[\"modified_email\"].str.len()) < MAX_CHAR_LEN\n].copy()\nskipped = len(dataset) - len(scoreable)\nprint(f\"Scoring {len(scoreable)} emails ({skipped} skipped — exceeded {MAX_CHAR_LEN} char limit)\")\n\n# Score each email\nreward_scores = []\nfor _, row in tqdm(scoreable.iterrows(), total=len(scoreable), desc=\"Scoring with Nemotron Reward\"):\n    user_msg = f\"Write a corporate email:\\n\\n{row['original_email']}\"\n    assistant_msg = row[\"modified_email\"]\n    score = get_reward_score(user_msg, assistant_msg)\n    reward_scores.append(score)\n\nscoreable[\"reward_score\"] = reward_scores\n\n# Merge scores back — entries that were skipped get NaN\ndataset = dataset.merge(\n    scoreable[[\"reward_score\"]],\n    left_index=True,\n    right_index=True,\n    how=\"left\",\n)\n\nprint(f\"\\nReward scores — mean: {dataset['reward_score'].mean():.3f}, \"\n      f\"std: {dataset['reward_score'].std():.3f}\")\nprint(f\"Entries with scores: {dataset['reward_score'].notna().sum()}\")\nprint(f\"Entries skipped (too long): {dataset['reward_score'].isna().sum()}\")\nprint(f\"\\nBy label:\")\nprint(dataset.groupby(\"is_malicious\")[\"reward_score\"].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet (efficient columnar format)\n",
    "dataset.to_parquet(\"enron_crime_injected.parquet\", index=False)\n",
    "\n",
    "# Save as CSV\n",
    "dataset.to_csv(\"enron_crime_injected.csv\", index=False)\n",
    "\n",
    "print(f\"Dataset saved: {len(dataset)} records\")\n",
    "print(f\"  - enron_crime_injected.parquet\")\n",
    "print(f\"  - enron_crime_injected.csv\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(dataset[\"is_malicious\"].value_counts())\n",
    "print(f\"\\nCrime distribution:\")\n",
    "print(dataset[dataset[\"is_malicious\"] == 1][\"crime_name\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}