{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying NVIDIA Nemotron-3-Nano with vLLM\n",
    "\n",
    "This notebook will walk you through how to run the `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B` model with vLLM.\n",
    "\n",
    "[vLLM](https://docs.vllm.ai) is a fast and easy-to-use library for LLM inference and serving. \n",
    "\n",
    "For more details on the model [click here](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8)\n",
    "\n",
    "Prerequisites:\n",
    "- NVIDIA GPU with recent drivers (‚â• 64 GB VRAM for BF16, ‚â• 32 GB for FP8, ‚â• 20 GB for NVFP4) and CUDA 12.x\n",
    "- Python 3.10+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch on NVIDIA Brev\n",
    "You can simplify the environment setup by using [NVIDIA Brev](https://developer.nvidia.com/brev). Click the button to launch this project on a Brev instance with the necessary dependencies pre-configured.\n",
    "\n",
    "Once deployed, click on the \"Open Notebook\" button to get started with this guide. \n",
    "\n",
    "**For H100 (BF16/FP8 models):**\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-36ikINrMffBCbrtTVLr6MFcllcs) \n",
    "\n",
    "**For RTX PRO 6000 (NVFP4 model - requires Blackwell architecture):**\n",
    "\n",
    "[![Launch on Brev](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-386KFyCvmg3y22JIf0q8BUh6jia) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpj3nyk2jv\n",
      "Processing /tmp/tmpj3nyk2jv/pip-25.0.1-py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-25.0.1\n"
     ]
    }
   ],
   "source": [
    "#If pip not found\n",
    "!python -m ensurepip --default-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install vllm torch nvidia-cutlass-dsl flashinfer-cubin==0.5.3 flashinfer-python==0.5.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Setup only for the NVFP4 Model (Blackwell GPUs):\n",
    "\n",
    "1. Install CUDA Toolkit and C++ compiler\n",
    "```shell\n",
    "sudo apt update\n",
    "sudo apt install -y cuda-toolkit-12-8 g++ gcc build-essential\n",
    "```\n",
    "\n",
    "2. Set environment variables (add to ~/.bashrc to make permanent)\n",
    "```shell\n",
    "export CUDA_HOME=/usr/local/cuda-12.8\n",
    "export PATH=$CUDA_HOME/bin:$PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify GPU\n",
    "\n",
    "Confirm CUDA is available and your GPU is visible to PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Num GPUs: 1\n",
      "GPU[0]: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "# GPU environment check\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Num GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU[{i}]: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Initialize the Nemotron model in vLLM with BF16 or FP8 (instructions for NVFP4 in the next cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadeform/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'disable_log_stats': True, 'model': 'nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-12 19:00:05 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-12 19:00:07,029\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-12 19:00:07 [model.py:637] Resolved architecture: NemotronHForCausalLM\n",
      "INFO 12-12 19:00:07 [model.py:1750] Using max model len 262144\n",
      "INFO 12-12 19:00:07 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 12-12 19:00:07 [config.py:315] Disabling cascade attention since it is not supported for hybrid models.\n",
      "INFO 12-12 19:00:07 [config.py:439] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.\n",
      "INFO 12-12 19:00:07 [config.py:463] Padding mamba page size by 1.13% to ensure that mamba page size and attention page size are exactly equal.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:08 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', speculative_config=None, tokenizer='nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.122.108:51693 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [gpu_model_runner.py:3467] Starting to load model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:09 [layer.py:379] Enabled separate cuda stream for MoE shared_experts\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:10 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:00<00:11,  1.03it/s]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:01<00:08,  1.34it/s]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:02<00:08,  1.20it/s]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:03<00:07,  1.18it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:04<00:07,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:05<00:06,  1.11it/s]\n",
      "Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:06<00:05,  1.14it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 8/13 [00:07<00:04,  1.12it/s]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 9/13 [00:07<00:03,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 10/13 [00:08<00:02,  1.13it/s]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 11/13 [00:09<00:01,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 12/13 [00:10<00:00,  1.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 13/13 [00:11<00:00,  1.13it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [default_loader.py:308] Loading weights took 11.60 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:22 [gpu_model_runner.py:3549] Model loading took 58.9076 GiB memory and 12.339329 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:655] Using cache directory: /home/shadeform/.cache/vllm/torch_compile_cache/b9f8ab6b7d/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:25 [backends.py:715] Dynamo bytecode transform time: 2.73 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:26 [backends.py:257] Cache the graph for dynamic shape for later use\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:27 [backends.py:288] Compiling a graph for dynamic shape takes 1.69 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:29 [fused_moe.py:875] Using configuration from /home/shadeform/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H100_80GB_HBM3.json for MoE layer.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:31 [monitor.py:34] torch.compile takes 4.43 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [gpu_worker.py:359] Available KV cache memory: 7.37 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m WARNING 12-12 19:00:32 [kv_cache_utils.py:1028] Add 1 padding layers, may waste at most 4.35% KV cache memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1286] GPU KV cache size: 257,280 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:32 [kv_cache_utils.py:1291] Maximum concurrency for 262,144 tokens per request: 4.82x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:13<00:00,  3.70it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51/51 [00:05<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [gpu_model_runner.py:4466] Graph capturing finished in 20 secs, took 1.39 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=55476)\u001b[0;0m INFO 12-12 19:00:52 [core.py:254] init engine (profile, create kv cache, warmup model) took 29.86 seconds\n",
      "INFO 12-12 19:00:54 [llm.py:343] Supported tasks: ['generate']\n",
      "Model ready\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\",\n",
    "    # Alternative: Load the FP8 quantized version for faster inference and lower memory usage\n",
    "    # model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-FP8\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the NVFP4 quantized version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variables for NVFP4\n",
    "os.environ[\"VLLM_USE_FLASHINFER_MOE_FP4\"] = \"1\"\n",
    "os.environ[\"VLLM_FLASHINFER_MOE_BACKEND\"] = \"throughput\"\n",
    "\n",
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4\",\n",
    "    tensor_parallel_size=1,\n",
    "    max_model_len=262144,\n",
    "    kv_cache_dtype=\"fp8\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"auto\",\n",
    "    max_num_seqs=8,\n",
    "    gpu_memory_utilization=0.85\n",
    ")\n",
    "\n",
    "print(\"Model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate responses\n",
    "\n",
    "Generate text with vLLM using single, batched, and simple streaming examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single or batch prompts\n",
    "\n",
    "Send one prompt or a list to run batched generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 366.73it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.45s/it, est. speed input: 0.47 toks/s, output: 8.53 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer in accordance with the format: your answer must contain exactly 3 bullet points. Use the markdown bullet points such as:\n",
      "* This is point 1. \n",
      "* This is point 2\n",
      "\n",
      "answer:\"\n",
      "\n",
      "We need to output exactly 3 bullet points, using markdown bullet points \"*\". So we need to give three bullet points about vLLM. Should be concise. Ensure exactly 3 bullet points, no extra text. No extra lines before or after? Probably just three bullet points. Ensure no extra bullet points or extra text. Provide exactly three lines each starting with \"* \". No extra blank lines. Let's produce:\n",
      "\n",
      "* vLLM is an open-source library for efficient large language model inference.\n",
      "* It supports high-throughput and low-latency serving via PagedAttention.\n",
      "* It enables easy scaling and deployment of LLMs across multiple GPUs and platforms.\n",
      "\n",
      "That's three bullet points. Ensure no extra text.\n",
      "</think>\n",
      "* vLLM is an open‚Äësource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 2030.48it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  1.60it/s, est. speed input: 9.60 toks/s, output: 235.75 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: 'Hello, my name is'\n",
      " NAME_1. Let's chat!\" The placeholder NAME_1 likely should be replaced with something? The user gave that as instruction; they might want the assistant to adopt that name? The instruction says \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" The example shows Bot says \"Hello, my name is NAME_1. Let's chat!\" So we should continue from that. The bot introduced itself as NAME_1. So we need to respond as Visitor? Or as Bot? The user says \"continue the conversation between Bot and Visitor.\" So we need to produce the next turn. The Bot already said greeting. So Visitor should reply. Probably we should respond as Visitor with a greeting and maybe ask how Bot is. Then Bot replies, etc. But the instruction: \"You are a chat bot, your goal is to continue the conversation between Bot and Visitor.\" So we need to output the next messages? Probably we\n",
      "\n",
      "Prompt 2: 'The capital of France is'\n",
      " Paris.\" with no extra words or explanation. This is a straightforward request, not disallowed. We can comply. Provide exactly that sentence, no extra words.\n",
      "</think>\n",
      "The capital of France is Paris.\n",
      "\n",
      "Prompt 3: 'Explain quantum computing in simple terms:'\n",
      " like a library with many books that can be in many states at once, etc. Provide analogies. Should be friendly. No disallowed content. Provide answer.\n",
      "</think>\n",
      "**Quantum Computing in a Nutshell (with a cozy knitting analogy)**  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Classical Computer: A Simple Stitch\n",
      "- **Bit = a single stitch** ‚Äì it can be either **0** (off) or **1** (on).  \n",
      "- By chaining many stitches together, a classical computer can represent any pattern, but each stitch has only one color at a time.\n",
      "\n",
      "### 2. The Quantum Computer: A Magical Yarn\n",
      "- **Qubit = a ‚Äúquantum stitch‚Äù** ‚Äì it can be **0, 1, or a blend of both** at the same time.  \n",
      "- In knitting terms, imagine a yarn that can be **half red, half blue, and also shimmering** all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "params = SamplingParams(temperature=0.6, max_tokens=200)\n",
    "\n",
    "# Single prompt\n",
    "single = llm.generate([\"Give me 3 bullet points about vLLM.\"], sampling_params=params)\n",
    "print(single[0].outputs[0].text)\n",
    "\n",
    "# Batch prompts\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"Explain quantum computing in simple terms:\"\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params=params)\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f\"\\nPrompt {i+1}: {out.prompt!r}\")\n",
    "    print(out.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamed generation\n",
    "\n",
    "Print characters as they are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1157.37it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.49s/it, est. speed input: 3.21 toks/s, output: 205.41 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Also mention \"Raiden Shogun\". Ensure haiku format: 5-7-5 syllables.\n",
      "\n",
      "We can produce:\n",
      "\n",
      "\"Silicon thunder,\n",
      "Raiden's will in silicon,\n",
      "Electrons pulse, swift.\"\n",
      "\n",
      "But that's not correct syllable count. Let's craft:\n",
      "\n",
      "\"Lightning cores ignite (5)\n",
      "Raiden's will on silicon (7)\n",
      "Sparks of fate arise (5)\n",
      "\n",
      "But need 5-7-5. Let's count.\n",
      "\n",
      "Line1: \"Lightning cores ignite\" -> Light-ning (2) cores (1) i-gnite (2) = 5? Let's count: Light(1) ning(1) = 2? Actually \"lightning\" is 2 syllables. \"cores\" 1, \"ignite\" 2 => total 5. Good.\n",
      "\n",
      "Line2: \"Raiden's will on silicon\" -> count: Ra-i-den's (3? Actually \"Raiden\" is 2 syllables? It's \"Ry-deen\"? Usually 2? In English \"Raiden\" is 2 syllables: \"Ry-den\". With possessive \"Raiden's\" still 2. \"will\" 1, \"on\" 1, \"silicon\" 3? \"si-li-con\" 3. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "Line3: \"Sparks of fate arise\" -> Sparks (1) of (1) fate (1) a-rise (2) = 5. Good.\n",
      "\n",
      "Thus haiku:\n",
      "\n",
      "Lightning cores ig"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nite  \n",
      "Raiden's will on silicon  \n",
      "Sparks of fate arise\n",
      "\n",
      "We can also mention \"GPU\" explicitly. Maybe \"GPU\" in line2? But we already have silicon. Could incorporate \"GPU\" but keep syllable count.\n",
      "\n",
      "Maybe:\n",
      "\n",
      "\"Silicon thunder (5?) Count: Si-li-con (3) thun-der (2) = 5. Good.\n",
      "\n",
      "\"Raiden's will in GPU\" Count: Ra-i-den's (2) will (1) in (1) G-P-U (3?) Actually \"GPU\" pronounced \"gee-pee-you\" 3 syllables. So total 2+1+1+3 = 7. Good.\n",
      "\n",
      "\"Electrons blaze\" Count: Elec-trons (3) blaze (1) = 4, need 5.\n"
     ]
    }
   ],
   "source": [
    "def stream_like(prompt: str, llm: LLM, sampling_params: SamplingParams) -> None:\n",
    "    outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
    "    text = outputs[0].outputs[0].text\n",
    "    print(\"Response:\", end=\" \")\n",
    "    for ch in text:\n",
    "        print(ch, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "stream_like(\"Write a haiku about GPUs.\", llm, SamplingParams(temperature=0.7, max_tokens=512))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI-compatible server\n",
    "\n",
    "Serve the model via an OpenAI-compatible API using vLLM.\n",
    "\n",
    "Before starting the server:\n",
    "- Restart the kernel to free GPU memory used by the in-process LLM\n",
    "- Ensure you use the same virtual environment with installed dependencies in your terminal. To do this within your Brev instance, open a terminal and run:\n",
    "  ```shell\n",
    "  source /home/shadeform/.venv/bin/activate\n",
    "  ```\n",
    "- Choose the desired model (FP8/BF16/NVFP4). The snippet below pulls the BF16 version (can be swapped with FP8). Follow the instructions in the next cell for NVFP4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After restarting the kernel, run this in a terminal:\n",
    "\n",
    "```shell\n",
    "git clone https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\n",
    "```\n",
    "\n",
    "```shell\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" \\\n",
    "    --dtype auto \\\n",
    "    --trust-remote-code \\\n",
    "    --served-model-name nemotron \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser qwen3_coder \\\n",
    "    --reasoning-parser-plugin \"NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/nano_v3_reasoning_parser.py\" \\\n",
    "    --reasoning-parser nano_v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NVFP4, restart the kernel and run this in a terminal:\n",
    "\n",
    "```shell\n",
    "git clone https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4\n",
    "```\n",
    "\n",
    "```shell\n",
    "VLLM_USE_FLASHINFER_MOE_FP4=1 \\\n",
    "VLLM_FLASHINFER_MOE_BACKEND=throughput \\\n",
    "vllm serve nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4 \\\n",
    "    --served-model-name nemotron \\\n",
    "    --max-num-seqs 8 \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --max-model-len 262144 \\\n",
    "    --kv-cache-dtype fp8 \\\n",
    "    --trust-remote-code \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 5000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser qwen3_coder \\\n",
    "    --reasoning-parser-plugin \"NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4/nano_v3_reasoning_parser.py\" \\\n",
    "    --reasoning-parser nano_v3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your server is now running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the API\n",
    "\n",
    "Send chat and streaming requests to your vLLM server using the OpenAI-compatible client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The model supports two modes - Reasoning ON (default) vs OFF. This can be toggled by setting enable_thinking to False, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client: Standard chat and streaming\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning on\n",
      "Reasoning: We need to output a haiku about GPUs. Haiku is 5-7-5 syllables, about GPUs.\n",
      "\n",
      "We'll produce a haiku.\n",
      "\n",
      "Need ensure correct syllable count.\n",
      "\n",
      "Potential haiku:\n",
      "\n",
      "\"Silicon heart thrums / parallel streams blaze night and day / fire forged in clay.\"\n",
      "\n",
      "Let's count syllables:\n",
      "\n",
      "Silicon-heart thrums = Si-li-con (3) heart (1) thrums (1) =5? Actually \"Silicon\" is 3 syllables (Si-li-con). \"heart\" is 1, \"thrums\" 1 => total 5. Good.\n",
      "\n",
      "parallel streams blaze night and day = par-allel (3) streams (1) blaze (1) night (1) and (1) day (1) =8? Let's count properly: \"parallel\" = 3 syllables (par-al-llel? Actually typically 3: par-al-lel). \"streams\" = 1, \"blaze\" =1, \"night\" =1, \"and\" =1, \"day\" =1 => total 3+1+1+1+1+1 =8 syllables. That's too many. Need 7.\n",
      "\n",
      " \n",
      "Content: None\n",
      "\n",
      "\n",
      "Reasoning off\n",
      "Here are 3 interesting facts about **vLLM** (a high-performance library for serving large language models):\n",
      "\n",
      "1. **PagedAttention: Revolutionizing Memory Management**  \n",
      "   vLLM introduces **PagedAttention**, a novel memory management technique inspired by virtual memory paging in operating systems. Instead of requiring contiguous GPU memory for KV caches (which causes fragmentation and limits batch sizes), it dynamically allocates memory blocks across the GPU. This allows vLLM to serve **2‚Äì4√ó more concurrent requests** with the same hardware (e.g., fitting 24K tokens on a single A100 vs. 6K in Hugging Face Transformers).\n",
      "\n",
      "2. **Native Support for Multi-Turn Conversations**  \n",
      "   Unlike many frameworks that require manual prompt engineering for chat history, vLLM natively handles **multi-turn dialogue** via its `ChatTemplate` system. It automatically structures prompts with system/user/assistant roles, manages context windows, and supports streaming responses‚Äîmaking it ideal for building chatbots without custom preprocessing.\n",
      "\n",
      "3. **Open-Source & Community-Driven Innovation**  \n",
      "   vLLM is **100% open-source** (Apache 2.0 license)\n"
     ]
    }
   ],
   "source": [
    "# Reasoning on (default)\n",
    "print(\"Reasoning on\")\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    ")\n",
    "print(\"Reasoning:\", resp.choices[0].message.reasoning_content, \"\\nContent:\", resp.choices[0].message.content)\n",
    "print(\"\\n\")\n",
    "# Reasoning off\n",
    "print(\"Reasoning off\")\n",
    "resp2 = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me 3 interesting facts about vLLM.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=256,\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "print(resp2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The first 5 prime numbers are:  \n",
      "**2, 3, 5, 7, 11**.  \n",
      "\n",
      "### Why?\n",
      "- **Prime numbers** are natural numbers greater than 1 that have no positive divisors other than 1 and themselves.\n",
      "- **2** is the smallest prime (and the only even prime).\n",
      "- **3**, **5**, **7**, and **11** follow as the next primes (4, 6, 8, 9, 10 are not prime).\n",
      "\n",
      "### Quick Check:\n",
      "| Number | Divisible by? | Prime? |\n",
      "|--------|---------------|--------|\n",
      "| 2      | 1, 2          | ‚úÖ Yes |\n",
      "| 3      | 1, 3          | ‚úÖ Yes |\n",
      "| 4      | 1, 2, 4       | ‚ùå No  |\n",
      "| 5      | 1, 5          | ‚úÖ Yes |\n",
      "| 6      | 1, 2, 3, 6    | ‚ùå No  |\n",
      "| 7      | 1, 7          | ‚úÖ Yes |\n",
      "| 8, 9, 10 | (not prime)   | ‚ùå No  |\n",
      "| **11** | **1, 11**     | ‚úÖ **Yes** |\n",
      "\n",
      "Thus, the sequence of the first 5 primes is **2, 3, 5, 7, 11**. üåü"
     ]
    }
   ],
   "source": [
    "# Streaming chat completion\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the first 5 prime numbers?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "Call functions using the OpenAI Tools schema and inspect returned tool_calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, the user wants to calculate a 15% tip on a $50 bill. Let me check the tools available. There's a calculate_tip function that takes bill_total and tip_percentage. The parameters are required, so I need both. The bill is $50, and the tip percentage is 15. I should call the function with these values. Let me make sure the parameters are integers. Yes, 50 and 15 are both integers. So the tool call should be calculate_tip with bill_total 50 and tip_percentage 15. That should give the tip amount.\n",
      "\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-b58e15d0f14b61c3', function=Function(arguments='{\"bill_total\": 50, \"tip_percentage\": 15}', name='calculate_tip'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Tool calling via OpenAI tools schema\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_tip\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"bill_total\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total amount of the bill\"\n",
    "                    },\n",
    "                    \"tip_percentage\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The percentage of tip to be applied\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"bill_total\", \"tip_percentage\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\"role\": \"user\", \"content\": \"My bill is $50. What will be the amount for 15% tip?\"}\n",
    "    ],\n",
    "    tools=TOOLS,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.reasoning_content)\n",
    "print(completion.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Reasoning Budget\n",
    "\n",
    "The `reasoning_budget` parameter allows you to limit the length of the model's reasoning trace. When the reasoning output reaches the specified token budget, the model will attempt to gracefully end the reasoning at the next newline character. \n",
    "\n",
    "If no newline is encountered within 500 tokens after reaching the budget threshold, the reasoning trace will be forcibly terminated at `reasoning_budget + 500` tokens to prevent excessive generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "import openai\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ThinkingBudgetClient:\n",
    "    def __init__(self, base_url: str, api_key: str, tokenizer_name_or_path: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "        self.client = openai.OpenAI(base_url=self.base_url, api_key=self.api_key)\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        reasoning_budget: int = 512,\n",
    "        max_tokens: int = 1024,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        assert (\n",
    "            max_tokens > reasoning_budget\n",
    "        ), f\"reasoning_budget must be smaller than max_tokens. Given {max_tokens=} and {reasoning_budget=}\"\n",
    "\n",
    "        # 1. first call chat completion to get reasoning content\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model, \n",
    "            messages=messages, \n",
    "            max_tokens=reasoning_budget, \n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "        reasoning_content = response.choices[0].message.reasoning_content or \"\"\n",
    "        \n",
    "        if \"</think>\" not in reasoning_content:\n",
    "            # reasoning content is too long, closed with a period (.)\n",
    "            reasoning_content = f\"{reasoning_content}.\\n</think>\\n\\n\"\n",
    "        \n",
    "        reasoning_tokens_used = len(\n",
    "            self.tokenizer.encode(reasoning_content, add_special_tokens=False)\n",
    "        )\n",
    "        remaining_tokens = max_tokens - reasoning_tokens_used\n",
    "        \n",
    "        assert (\n",
    "            remaining_tokens > 0\n",
    "        ), f\"remaining tokens must be positive. Given {remaining_tokens=}. Increase max_tokens or lower reasoning_budget.\"\n",
    "\n",
    "        # 2. append reasoning content to messages and call completion\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reasoning_content})\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            continue_final_message=True,\n",
    "        )\n",
    "        \n",
    "        response = self.client.completions.create(\n",
    "            model=model, \n",
    "            prompt=prompt, \n",
    "            max_tokens=remaining_tokens, \n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        response_data = {\n",
    "            \"reasoning_content\": reasoning_content.strip().strip(\"</think>\").strip(),\n",
    "            \"content\": response.choices[0].text,\n",
    "            \"finish_reason\": response.choices[0].finish_reason,\n",
    "        }\n",
    "        return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client\n",
    "client = ThinkingBudgetClient(\n",
    "    base_url=\"http://127.0.0.1:5000/v1\",\n",
    "    api_key=\"null\",\n",
    "    tokenizer_name_or_path=\"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: Okay, user wants a haiku about GPUs. Hmm, they probably want something concise but evocative - haikus are tricky with only 5-7. \n",
      "Content: \n",
      "Silent fire burns,  \n",
      "Parallel heat solves complex math‚Äî  \n",
      "Frames rise like dawn light.  \n",
      "\n",
      "*(Haiku structure: 5-7-5 syllables.  \n",
      "\"Silent fire burns\" (5) - captures GPU's quiet power and thermal intensity.  \n",
      "\"Parallel heat solves complex math\" (7) - nods to GPU architecture and computational work.  \n",
      "\"Frames rise like dawn light\" (5) - evokes rendering magic and visual payoff.)*\n"
     ]
    }
   ],
   "source": [
    "resp = client.chat_completion(\n",
    "    model=\"nemotron\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about GPUs.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=256,\n",
    "    reasoning_budget=32\n",
    ")\n",
    "print(\"Reasoning:\", resp[\"reasoning_content\"], \"\\nContent:\", resp[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Reasoning Budget Via Logit Processors\n",
    "\n",
    "An alternative to budget control shown above it to use VLLM's logit processor functionality. This method allows the client to avoid calling the server twice. \n",
    "\n",
    "To use this method you first need to install `custom_logit_processors` provided in this repository under `./tools/budget`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd ./tools/budget && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, you can launch a VLLM server and point the `custom_logit_processors` via the commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%env THINKING_BUDGET_LOGITS_PROCESSOR_ARGS={\"thinking_budget\":150,\"thinking_budget_grace_period\":30,\"end_token_ids\":[1338,13],\"end_think_ids\":[[13]],\"prompt_think_ids\":[12,1010]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up default arguments to the logit processor. In the above example: \n",
    "\n",
    "`thinking_budget` is the number of tokens the model can use in thinking/reasoning stage. \n",
    "`thinking_budget_grace_period` extra number of tokens after the budget to find a newline to gracefully stop thinking \n",
    "`end_token_ids` the sequence of tokens to artificially insert into the token stream to end thinking\n",
    "`end_think_ids` the id for </think> (always 13 for this model) \n",
    "`prompt_think_ids` the sequence to ids to allow the logit processor to recognize that the model is in thinking stage (always [12, 1010] for this model) \n",
    "\n",
    "Now we can launch the VLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --served-model-name \"model\" \\\n",
    "  --model nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 \\\n",
    "  --logits-processors \"custom_logit_processors.v1.nano_v3_logit_processors:ThinkingBudgetLogitsProcessor\" \\\n",
    "  --port 8881 \\\n",
    "  --trust-remote-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server is running, we can use the client to control the reasoning budget.\n",
    "An example client is provided in `./tools/budget/client.py`. \n",
    "\n",
    "We can use the client without any additional arguments to use the default thinking budget which was set when the VLLM server was launched..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8881/v1\", # Your vLLM server URL\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "\n",
    "result = client.chat.completions.create(\n",
    "    model=\"model\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Consider all the ways you can interpret the question 'What is 5.9 plus 6.1' and give the best answer possible.\"}\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    max_tokens=12200, # uses the default thinking budget set during starting of the vllm server.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will trigger the default truncation behavior around 150 tokens  + 30 grace tokens. The output of the reasoning part should look something like this:\n",
    "\n",
    "The user wants \"Consider all the ways you can interpret the question 'What is 5.9 plus 6.1' and give the best answer possible.\"\n",
    "\n",
    "We need to interpret the phrase in various ways ‚Äì maybe as basic arithmetic, as concatenation, as some alternative base, maybe as a trick question. Provide the best answer possible.\n",
    "\n",
    "We need to be thorough: cover decimal addition, interpretation in base X, rounding, approximation, maybe interpretation as time (5.9 seconds + 6.1 seconds) = 12.0 seconds, maybe as different numeral systems, as measurement with units.\n",
    "\n",
    "The question likely wants all possible interpretations and then provide the best answer.\n",
    "\n",
    "Thus: interpret as simple addition in base 10 yields 12.0. Could also be interpreted as 5.9 + 6.1 = 12.0 exactly because 0..\n",
    "\n",
    "</think>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client may alter the defaults during each call as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_think_budget = 10\n",
    "custom_think_budget_grace_period = 10\n",
    "custom_think_truncation = [1871, 5565, 11483, 6139, 2016, 1536, 6934, 1338, 13] # \"Reached thining limit set by client\\n\\n</think>\n",
    "\n",
    "result = client.chat.completions.create(\n",
    "    model=\"model\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Consider all the ways you can interpret the question 'What is 5.9 plus 6.1' and give the best answer possible.\"}\n",
    "    ],\n",
    "    temperature=1.0,\n",
    "    max_tokens=12200,\n",
    "    logprobs=False,\n",
    "    extra_body={\n",
    "        \"vllm_xargs\": {\n",
    "            \"thinking_budget\": custom_think_budget,\n",
    "            \"thinking_budget_grace_period\": custom_think_budget_grace_period,\n",
    "            \"end_token_ids\": json.dumps(custom_think_truncation),\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in a reasoning trace rather brutally truncated like this:\n",
    "\n",
    "The user asks: \"Consider all the ways you can interpret the question 'What is 5. Reached thinking limit set by client.\n",
    "\n",
    "</think>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
