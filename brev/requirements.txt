# Brev / local GGUF + llama.cpp runtime
# Install with: pip install -r requirements.txt
# For GPU (Brev): CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install -r requirements.txt

huggingface-hub>=0.20.0
llama-cpp-python[server]>=0.3.0
